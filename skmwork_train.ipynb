{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SKM-TEA from main branch on GitHub\n",
    "!pip install --upgrade pytorch-lightning==1.7.7 skm-tea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import label2rgb\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "\n",
    "import dosma as dm\n",
    "\n",
    "import meddlr.ops as oF\n",
    "from meddlr.data import DatasetCatalog, MetadataCatalog\n",
    "from meddlr.utils.logger import setup_logger\n",
    "from meddlr.utils import env\n",
    "\n",
    "import skm_tea as st\n",
    "# import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "from meddlr.data.data_utils import collect_mask\n",
    "\n",
    "from typing import Union, Sequence\n",
    "\n",
    "import os,sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "\n",
    "sys.path.append(os.getcwd)\n",
    "print(sys.path)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "  DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device: \", DEVICE)\n",
    "\n",
    "torch.cuda.set_device(1) \n",
    "\n",
    "\n",
    "image_names = os.listdir('./v1-release/image_files')\n",
    "folder_path = './v1-release/image_files'\n",
    "\n",
    "image_names_train = image_names[:5]\n",
    "image_names_test = image_names[6:]\n",
    "\n",
    "print(\"train:\", image_names_train, end='  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_image(\n",
    "        x: Union[torch.Tensor, np.ndarray], percentile=0.99, clip=False\n",
    "):\n",
    "    \"\"\"Scales image by intensity percentile (and optionally clips to [0, 1]).\n",
    "\n",
    "    Args:\n",
    "      x (torch.Tensor | np.ndarray): The image to process.\n",
    "      percentile (float): The percentile of magnitude to scale by.\n",
    "      clip (bool): If True, clip values between [0, 1]\n",
    "\n",
    "    Returns:\n",
    "      torch.Tensor | np.ndarray: The scaled image.\n",
    "    \"\"\"\n",
    "    is_numpy = isinstance(x, np.ndarray)\n",
    "    if is_numpy:\n",
    "        x = torch.as_tensor(x)\n",
    "\n",
    "    scale_factor = torch.quantile(x, percentile)\n",
    "    x = x / scale_factor\n",
    "    if clip:\n",
    "        x = torch.clip(x, 0, 1)\n",
    "\n",
    "    if is_numpy:\n",
    "        x = x.numpy()\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def plot_images(\n",
    "        images, processor=None, disable_ticks=True, titles: Sequence[str] = None,\n",
    "        ylabel: str = None, xlabels: Sequence[str] = None, cmap: str = \"gray\",\n",
    "        show_cbar: bool = False, overlay=None, opacity: float = 0.3,\n",
    "        hsize=5, wsize=5, axs=None\n",
    "):\n",
    "    \"\"\"Plot multiple images in a single row.\n",
    "\n",
    "    Add an overlay with the `overlay=` argument.\n",
    "    Add a colorbar with `show_cbar=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_default_values(x, default=\"\"):\n",
    "        if x is None:\n",
    "            return [default] * len(images)\n",
    "        return x\n",
    "\n",
    "    titles = get_default_values(titles)\n",
    "    ylabels = get_default_values(images)\n",
    "    xlabels = get_default_values(xlabels)\n",
    "\n",
    "    N = len(images)\n",
    "    if axs is None:\n",
    "        fig, axs = plt.subplots(1, N, figsize=(wsize * N, hsize))\n",
    "    else:\n",
    "        assert len(axs) >= N\n",
    "        fig = axs.flatten()[0].get_figure()\n",
    "\n",
    "    for ax, img, title, xlabel in zip(axs, images, titles, xlabels):\n",
    "        if processor is not None:\n",
    "            img = processor(img)\n",
    "        im = ax.imshow(img, cmap=cmap)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(xlabel)\n",
    "\n",
    "    if overlay is not None:\n",
    "        for ax in axs.flatten():\n",
    "            im = ax.imshow(overlay, alpha=opacity)\n",
    "\n",
    "    if show_cbar:\n",
    "        fig.subplots_adjust(right=0.8)\n",
    "        cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "        fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "    if disable_ticks:\n",
    "        for ax in axs.flatten():\n",
    "            ax.get_xaxis().set_ticks([])\n",
    "            ax.get_yaxis().set_ticks([])\n",
    "\n",
    "    return axs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def CE_Loss(inputs, target, cls_weights, num_classes=21):\n",
    "    \"\"\"\"\"\"\n",
    "    # n, c, d, h, w = inputs.size()  # V-Net\n",
    "    n, c, h, w = inputs.size()       # U-Net\n",
    "    nt, ht, wt = target.size()\n",
    "    if h != ht or w != wt:\n",
    "        inputs = F.interpolate(inputs, size=(ht, wt), mode='bilinear', align_corners=True)\n",
    "\n",
    "    # temp_inputs = inputs.transpose(1, 2).transpose(2, 3).transpose(3, 4).contiguous().view(-1, c)   # V-Net\n",
    "    temp_inputs = inputs.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)                     # U-Net\n",
    "    temp_target = target.contiguous().view(-1)\n",
    "\n",
    "    CE_loss = nn.CrossEntropyLoss(weight=cls_weights, ignore_index=num_classes)(temp_inputs, temp_target)\n",
    "    \n",
    "    return CE_loss\n",
    "\n",
    "def BCE_Loss(inputs, target, vnet=False):\n",
    "    \"\"\"\"\"\"\n",
    "    if vnet:\n",
    "        n, c, d, h, w = inputs.size()  # V-Net\n",
    "    else:\n",
    "        n, c, h, w = inputs.size()       # U-Net\n",
    "    nt, ht, wt ,ct= target.size()\n",
    "    if h != ht or w != wt:\n",
    "        inputs = F.interpolate(inputs, size=(ht, wt), mode='bilinear', align_corners=True)\n",
    "\n",
    "    if vnet:\n",
    "        temp_inputs = inputs.transpose(1, 2).transpose(2, 3).transpose(3, 4).contiguous().view(-1, c)     # V-Net\n",
    "    else:\n",
    "        temp_inputs = inputs.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)                     # U-Net\n",
    "    temp_target = target.contiguous().view(-1, ct).float()\n",
    "\n",
    "    BCE_loss = nn.BCEWithLogitsLoss()(temp_inputs, temp_target)\n",
    "     \n",
    "    return BCE_loss\n",
    "\n",
    "def Dice_Loss(inputs, target, beta=1, smooth=1e-5):\n",
    "    \"\"\"\n",
    "        Dice loss: 1 - 2TP / (2TP + FP + FN)\n",
    "    \"\"\"\n",
    "    n, c, h, w = inputs.size()\n",
    "    nt, ht, wt, ct = target.size()\n",
    "    if h != ht or w != wt:\n",
    "        inputs = F.interpolate(inputs, size=(ht, wt), mode='bilinear', align_corners=True)\n",
    "\n",
    "    # temp_inputs = torch.softmax(inputs.transpose(1, 2).transpose(2, 3).contiguous().view(n, -1, c), dim=-1)\n",
    "    temp_inputs = torch.sigmoid(inputs.transpose(1, 2).transpose(2, 3).contiguous().view(n, -1, c))\n",
    "    temp_target = target.view(n, -1, ct)\n",
    "\n",
    "    tp = torch.sum(temp_inputs * temp_target, dim=(0, 1))\n",
    "    fp = torch.sum(temp_inputs, dim=(0, 1)) - tp\n",
    "    fn = torch.sum(temp_target, dim=(0, 1)) - tp\n",
    "\n",
    "    score = ((1 + beta ** 2) * tp + smooth) / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + smooth)\n",
    "    dice_loss = 1 - torch.mean(score)\n",
    "    return dice_loss\n",
    "\n",
    "\n",
    "def weights_init(net, init_type='normal', init_gain=0.02):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and classname.find('Conv') != -1:\n",
    "            if init_type == 'normal':\n",
    "                torch.nn.init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == 'xavier':\n",
    "                torch.nn.init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                torch.nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                torch.nn.init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    print('initialize network with %s type' % init_type)\n",
    "    net.apply(init_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_32(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet_32, self).__init__()\n",
    "        \n",
    "        # Contracting Path (Encoder)\n",
    "        self.enc_conv0 = self.conv_stage(1, 32)  # New initial layer with 32 channels\n",
    "        self.enc_conv1 = self.conv_stage(32, 64)\n",
    "        self.enc_conv2 = self.conv_stage(64, 128)\n",
    "        self.enc_conv3 = self.conv_stage(128, 256)\n",
    "        self.enc_conv4 = self.conv_stage(256, 512)\n",
    "        self.enc_conv5 = self.conv_stage(512, 1024)\n",
    "\n",
    "        # Expanding Path (Decoder)\n",
    "        self.dec_conv4 = self.conv_stage(1024 , 512)\n",
    "        self.dec_conv3 = self.conv_stage(512, 256)\n",
    "        self.dec_conv2 = self.conv_stage(256 , 128)\n",
    "        self.dec_conv1 = self.conv_stage(128 , 64)\n",
    "        self.dec_conv0 = self.conv_stage(64 , 32)  # New final layer with 32 channels\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.upconv0 = nn.ConvTranspose2d(64, 32, 2, stride=2)  # New upsampling layer\n",
    "\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.BatchNorm2d4 = nn.BatchNorm2d(512)\n",
    "        self.BatchNorm2d3 = nn.BatchNorm2d(256)\n",
    "        self.BatchNorm2d2 = nn.BatchNorm2d(128)\n",
    "        self.BatchNorm2d1 = nn.BatchNorm2d(64)\n",
    "        self.BatchNorm2d0 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Final Output\n",
    "        self.final_conv = nn.Conv2d(32, 4, 1)\n",
    "\n",
    "    def conv_stage(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            # nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc0 = self.enc_conv0(x)\n",
    "        x = F.max_pool2d(enc0, 2)\n",
    "        enc1 = self.enc_conv1(x)\n",
    "        x = F.max_pool2d(enc1, 2)\n",
    "        enc2 = self.enc_conv2(x)\n",
    "        x = F.max_pool2d(enc2, 2)\n",
    "        enc3 = self.enc_conv3(x)\n",
    "        x = F.max_pool2d(enc3, 2)\n",
    "        enc4 = self.enc_conv4(x)\n",
    "        x = F.max_pool2d(enc4, 2)\n",
    "        x = self.enc_conv5(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.upconv4(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.BatchNorm2d4(x) \n",
    "        x = torch.cat((x, enc4), dim=1)\n",
    "        x = self.dec_conv4(x)\n",
    "\n",
    "        x = self.upconv3(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.BatchNorm2d3(x) \n",
    "        x = torch.cat((x, enc3), dim=1)\n",
    "        x = self.dec_conv3(x)\n",
    "\n",
    "        x = self.upconv2(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.BatchNorm2d2(x) \n",
    "        x = torch.cat((x, enc2), dim=1)\n",
    "        x = self.dec_conv2(x)\n",
    "\n",
    "        x = self.upconv1(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.BatchNorm2d1(x) \n",
    "        x = torch.cat((x, enc1), dim=1)\n",
    "        x = self.dec_conv1(x)\n",
    "\n",
    "        x = self.upconv0(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.BatchNorm2d0(x)\n",
    "        x = torch.cat((x, enc0), dim=1)\n",
    "        x = self.dec_conv0(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VNet_80(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VNet_80, self).__init__()\n",
    "        self.down1 = self._conv_block(1, 16)\n",
    "        self.down2 = self._conv_block(16, 32)\n",
    "        self.down3 = self._conv_block(32, 64)\n",
    "        self.down4 = self._conv_block(64, 128)\n",
    "        # No down5 due to reduced depth\n",
    "        \n",
    "        self.up4 = self._up_block(128, 64)\n",
    "        self.up3 = self._up_block(64, 32)\n",
    "        self.up2 = self._up_block(32, 16)\n",
    "        \n",
    "        self.final_conv = nn.Conv3d(16, 4, kernel_size=1)\n",
    "\n",
    "    def _conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def _up_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(F.max_pool3d(x1, 2))\n",
    "        x3 = self.down3(F.max_pool3d(x2, 2))\n",
    "        x4 = self.down4(F.max_pool3d(x3, 2))\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.up4(x4)\n",
    "        x = torch.add(x, x3)\n",
    "        x = self.up3(x)\n",
    "        x = torch.add(x, x2)\n",
    "        x = self.up2(x)\n",
    "        x = torch.add(x, x1)\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "# [16,16,1024]  -> [256, 1024]\n",
    "\n",
    "# dim = 1024\n",
    "# depth = 12\n",
    "# heads = 8\n",
    "# dim_head = 64\n",
    "# mlp_dim = 150\n",
    "# dropout = 0.2\n",
    "# Utransformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "\n",
    "class TransNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransNet, self).__init__()\n",
    "        \n",
    "        # Contracting Path (Encoder)\n",
    "        self.enc_conv0 = self.conv_stage(1, 32)  # New initial layer with 32 channels\n",
    "        self.enc_conv1 = self.conv_stage(32, 64)\n",
    "        self.enc_conv2 = self.conv_stage(64, 128)\n",
    "        self.enc_conv3 = self.conv_stage(128, 256)\n",
    "        self.enc_conv4 = self.conv_stage(256, 512)\n",
    "        self.enc_conv5 = self.conv_stage(512, 1024)\n",
    "\n",
    "        # Expanding Path (Decoder)\n",
    "        self.dec_conv4 = self.conv_stage(1024 , 512)\n",
    "        self.dec_conv3 = self.conv_stage(512, 256)\n",
    "        self.dec_conv2 = self.conv_stage(256 , 128)\n",
    "        self.dec_conv1 = self.conv_stage(128 , 64)\n",
    "        self.dec_conv0 = self.conv_stage(64 , 32)  # New final layer with 32 channels\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.upconv0 = nn.ConvTranspose2d(64, 32, 2, stride=2)  # New upsampling layer\n",
    "\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.BatchNorm2d4 = nn.BatchNorm2d(512)\n",
    "        self.BatchNorm2d3 = nn.BatchNorm2d(256)\n",
    "        self.BatchNorm2d2 = nn.BatchNorm2d(128)\n",
    "        self.BatchNorm2d1 = nn.BatchNorm2d(64)\n",
    "        self.BatchNorm2d0 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Final Output\n",
    "        self.final_conv = nn.Conv2d(32, 4, 1)\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            nn.LayerNorm(1024),\n",
    "            # nn.Linear(1024, 768),\n",
    "            # nn.LayerNorm(768),\n",
    "        )\n",
    "\n",
    "        dim = 1024\n",
    "        depth = 8\n",
    "        heads = 6\n",
    "        dim_head = 32\n",
    "        mlp_dim = 768\n",
    "        dropout = 0.2\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "    def conv_stage(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            # nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder [512,512,1]\n",
    "        enc0 = self.enc_conv0(x)    #[512,512,32]\n",
    "        x = F.max_pool2d(enc0, 2)   #[256,256,32]\n",
    "        enc1 = self.enc_conv1(x)    #[256,256,64]\n",
    "        x = F.max_pool2d(enc1, 2)   #[128,128,64]\n",
    "        enc2 = self.enc_conv2(x)    #[128,128,128]\n",
    "\n",
    "        x = F.max_pool2d(enc2, 2)   #[64,64,128]\n",
    "        enc3 = self.enc_conv3(x)\n",
    "        x = F.max_pool2d(enc3, 2)\n",
    "        enc4 = self.enc_conv4(x)\n",
    "        x = F.max_pool2d(enc4, 2)\n",
    "        x = self.enc_conv5(x)\n",
    "        # x = x.reshape(x.shape[0], 16, 16, 1024)\n",
    "        x = x.permute(0, 2, 3, 1)  \n",
    "        # print(x.shape)\n",
    "\n",
    "        # Transformer\n",
    "        x = x.view(x.shape[0], 256, 1024) \n",
    "        x = self.to_patch_embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.view(x.shape[0], 16, 16, 1024) \n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.upconv4(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.BatchNorm2d4(x) \n",
    "        x = torch.cat((x, enc4), dim=1)\n",
    "        x = self.dec_conv4(x)\n",
    "\n",
    "        x = self.upconv3(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.BatchNorm2d3(x) \n",
    "        x = torch.cat((x, enc3), dim=1)\n",
    "        x = self.dec_conv3(x)\n",
    "\n",
    "        x = self.upconv2(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.BatchNorm2d2(x) \n",
    "        x = torch.cat((x, enc2), dim=1)\n",
    "        x = self.dec_conv2(x)\n",
    "\n",
    "        x = self.upconv1(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.BatchNorm2d1(x) \n",
    "        x = torch.cat((x, enc1), dim=1)\n",
    "        x = self.dec_conv1(x)\n",
    "\n",
    "        x = self.upconv0(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.BatchNorm2d0(x)\n",
    "        x = torch.cat((x, enc0), dim=1)\n",
    "        x = self.dec_conv0(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prapare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo1_o = []\n",
    "segmentations_o = []\n",
    "\n",
    "for h5_file in tqdm(image_names_train,desc='process dataset: '):\n",
    "# 拼接完整的文件路径\n",
    "    image_file = os.path.join(folder_path, h5_file)\n",
    "    with h5py.File(image_file, \"r\") as f:\n",
    "        echo1_f = f[\"echo1\"][()]  # Shape: (x, y, z)\n",
    "        echo2 = f[\"echo2\"][()]  # Shape: (x, y, z)\n",
    "        segmentations_f = f[\"seg\"][()]  # Shape: (x, y, z, #classes)\n",
    "        \n",
    "    # print(echo1_f.shape[2])\n",
    "    # if echo1_f.shape[2] == 160:\n",
    "    echo1_o.append(echo1_f)\n",
    "    segmentations_o.append(segmentations_f)\n",
    "\n",
    "\n",
    "echo1_o = np.concatenate(echo1_o,axis=2)\n",
    "segmentations_o = np.concatenate(segmentations_o,axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "batch_size = 40\n",
    "sum_epoch = 100\n",
    "pretrained = False\n",
    "lr = 1e-3\n",
    "CHECKPOINT = False\n",
    "dice_loss = False\n",
    "save_modelpath = \"./save_model/\"\n",
    "save_checkPath = './checkPoint_model_bce/'\n",
    "save_path = os.path.join(save_modelpath, 'unet32_model_100_BCE.pth')\n",
    "scalar_name = \"aver_Loss_bce\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet_32()\n",
    "\n",
    "if not pretrained:\n",
    "    weights_init(model)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(os.path.join('./save_model/unet32_model_100.pth'),map_location=DEVICE))\n",
    "    print(\"Load model success! \")\n",
    "\n",
    "print(model)\n",
    "print('------------------')\n",
    "# print_model_info(model)\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr, weight_decay=5e-3)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.94)\n",
    "start_epoch = 0\n",
    "\n",
    "if CHECKPOINT:\n",
    "    path_checkpoint = os.path.join(save_checkPath, 'ckpt_best_120.pth')\n",
    "    checkpoint = torch.load(path_checkpoint)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = echo1_o.shape[2]\n",
    "x_chunks = np.array_split(echo1_o, total / batch_size, axis=2)\n",
    "segmentation_chunks = np.array_split(segmentations_o, total / batch_size, axis=2)\n",
    "\n",
    "tensorboardPath = \"./runs\"\n",
    "if not os.path.isdir(tensorboardPath):\n",
    "    os.mkdir(tensorboardPath)\n",
    "writer = SummaryWriter(log_dir=tensorboardPath)\n",
    "\n",
    "for epoch in range(start_epoch, sum_epoch):\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    print('Start Train')\n",
    "\n",
    "    bat = 0\n",
    "    with tqdm(total=len(x_chunks), desc=f'Epoch {epoch+1}/{sum_epoch}', postfix=dict) as pbar:\n",
    "        for chunk in x_chunks:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                chunk =  torch.as_tensor(chunk).float().to(DEVICE)\n",
    "                chunk = (chunk - chunk.mean()) / chunk.std()\n",
    "\n",
    "                chunk = chunk.permute(2, 0, 1)  #(B,H,W) (40,512,512)\n",
    "                chunk = chunk.unsqueeze(1)      # add a channel dimension (B,C,H,W) (40,1,512,512)\n",
    "                \n",
    "                segmentation = segmentation_chunks[bat]                                                            #[512,512,40,6]\n",
    "                segmentation = collect_mask(segmentation, (0, 1, (2, 3), (4, 5)), out_channel_first=False)         #[512,512,40,4]\n",
    "                # segmentation = oF.one_hot_to_categorical(segmentation, channel_dim=-1)   #label  [1 1 1 ... 2 2 2]   [512,512,40]\n",
    "\n",
    "                segmentation =  torch.as_tensor(segmentation).to(DEVICE)\n",
    "                segmentation = segmentation.permute(2, 0, 1, 3)             #[40,512,512,4]\n",
    "\n",
    "            pre_out = model(chunk)                                          #[40,4,512,512]\n",
    "            # pre_out = oF.pred_to_categorical(pre, activation='sigmoid')   # [40,512,512]\n",
    "\n",
    "            if dice_loss:\n",
    "                loss = Dice_Loss(pre_out, segmentation)\n",
    "            else:\n",
    "                loss = BCE_Loss(pre_out, segmentation)\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            bat = bat + 1\n",
    "            pbar.set_postfix(**{\n",
    "            'cur_loss': loss.item(),\n",
    "            'averange_loss': total_loss / bat\n",
    "            })\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    writer.add_scalars(scalar_name, {\"Train\": total_loss / bat }, epoch)\n",
    "\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch':(epoch+1),\n",
    "            'model':model.state_dict(),\n",
    "            'optimizer':optimizer.state_dict(),\n",
    "            'lr_scheduler':lr_scheduler.state_dict()}\n",
    "        if not os.path.isdir(save_checkPath):\n",
    "            os.mkdir(save_checkPath)\n",
    "        torch.save(checkpoint,save_checkPath+'/ckpt_best_%s.pth'%(str(epoch+1)))\n",
    "\n",
    "# Save  \n",
    "# save_path = os.path.join(save_modelpath, 'unet32_model_200_BCE.pth')\n",
    "if not os.path.isdir(save_path):\n",
    "    os.mkdir(save_path)\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train V-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prapare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo1_o = []\n",
    "segmentations_o = []\n",
    "\n",
    "for h5_file in tqdm(image_names_train,desc='process dataset: '):\n",
    "\n",
    "    image_file = os.path.join(folder_path, h5_file)\n",
    "    with h5py.File(image_file, \"r\") as f:\n",
    "        echo1_f = f[\"echo1\"][()]  # Shape: (x, y, z)\n",
    "        echo2 = f[\"echo2\"][()]  # Shape: (x, y, z)\n",
    "        segmentations_f = f[\"seg\"][()]  # Shape: (x, y, z, #classes)\n",
    "        \n",
    "    # print(echo1_f.shape[2])\n",
    "    if echo1_f.shape[2] == 160:\n",
    "        echo1_o.append(echo1_f)\n",
    "        segmentations_o.append(segmentations_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "batch_size = 40\n",
    "sum_epoch = 100\n",
    "pretrained = False\n",
    "lr = 1e-3\n",
    "CHECKPOINT = False\n",
    "dice_loss = False\n",
    "save_modelpath = \"./save_model/\"\n",
    "if not os.path.isdir(save_modelpath):\n",
    "    os.mkdir(save_modelpath)\n",
    "save_checkPath = './checkPoint_Vmodel_bce/'\n",
    "save_path = os.path.join(save_modelpath, 'Vnet80_model_100_BCE.pth')\n",
    "scalar_name = \"Vnet_aver_Loss_bce\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VNet_80()\n",
    "\n",
    "if not pretrained:\n",
    "    weights_init(model)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(os.path.join('./save_model/Vnet80_model_100.pth'),map_location=DEVICE))\n",
    "    print(\"Load model success! \")\n",
    "\n",
    "print(model)\n",
    "print('------------------')\n",
    "# print_model_info(model)\n",
    "model = model.cuda()\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr, weight_decay=5e-3)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.94)\n",
    "start_epoch = 0\n",
    "\n",
    "if CHECKPOINT:\n",
    "    path_checkpoint = os.path.join(save_checkPath, 'ckpt_best_120.pth')\n",
    "    checkpoint = torch.load(path_checkpoint)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboardPath = \"./runs\"\n",
    "if not os.path.isdir(tensorboardPath):\n",
    "    os.mkdir(tensorboardPath)\n",
    "writer = SummaryWriter(log_dir=tensorboardPath)\n",
    "\n",
    "for epoch in range(sum_epoch):\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    print('Start Train')\n",
    "\n",
    "    bat = 0\n",
    "    with tqdm(total=len(echo1_o), desc=f'Epoch {epoch+1}/{sum_epoch}', postfix=dict) as pbar:\n",
    "        for echo1 in echo1_o:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                x_chunks = np.array_split(echo1, 4, axis=2)\n",
    "                segmentation_chunks = np.array_split(segmentations_o[bat], 4, axis=2)\n",
    "\n",
    "            sl = 0\n",
    "            for x_chunk in x_chunks:\n",
    "                with torch.no_grad():\n",
    "                    chunk =  torch.as_tensor(x_chunk).float().to('cuda')\n",
    "                    # chunk = chunk.cuda()\n",
    "                    chunk = (chunk - chunk.mean()) / chunk.std()    #(512,512,160)\n",
    "\n",
    "\n",
    "                    chunk = chunk.permute(2, 0, 1)               # (B,H,W) (160,512,512)\n",
    "                    chunk = chunk.unsqueeze(0).unsqueeze(0)      # add a channel dimension (B,C,H,W) (1,1,160,512,512)\n",
    "\n",
    "                    # segmentation = segmentations_o[bat]                #(x,y,160,6)\n",
    "                    segmentation = collect_mask(segmentation_chunks[sl], (0, 1, (2, 3), (4, 5)), out_channel_first=False)          #(x,y,160,4)\n",
    "                    # segmentation = oF.one_hot_to_categorical(segmentation, channel_dim=-1)   #label  [1 1 1 ... 2 2 2]   (x,y,160)\n",
    "\n",
    "                    segmentation =  torch.as_tensor(segmentation).to('cuda')\n",
    "                    segmentation = segmentation.permute(2, 0, 1, 3)           #(160,x,y,4)\n",
    "\n",
    "\n",
    "                pre_out = model(chunk)  #(1,4,40,512,512)\n",
    "                # logits.append(out[\"sem_seg_logits\"])\n",
    "                # segmentation_chunk = segmentation_chunks[sl]\n",
    "                if dice_loss:\n",
    "                    loss = Dice_Loss(pre_out, segmentation, vnet=True)\n",
    "                else:\n",
    "                    loss = BCE_Loss(pre_out, segmentation, vnet=True)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                sl += sl\n",
    "\n",
    "            bat = bat + 1\n",
    "            pbar.set_postfix(**{\n",
    "                'cur_loss': loss.item(),\n",
    "                'averange_loss': total_loss / bat\n",
    "                })\n",
    "                \n",
    "            pbar.update(1)\n",
    "            \n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    writer.add_scalars(scalar_name, {\"Train\": total_loss / bat }, epoch)\n",
    "\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch':(epoch+1),\n",
    "            'model':model.state_dict(),\n",
    "            'optimizer':optimizer.state_dict(),\n",
    "            'lr_scheduler':lr_scheduler.state_dict()}\n",
    "        if not os.path.isdir(save_checkPath):\n",
    "            os.mkdir(save_checkPath)\n",
    "        torch.save(checkpoint,save_checkPath+'/ckpt_best_%s.pth'%(str(epoch+1)))\n",
    "\n",
    "\n",
    "# Save\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Transformer-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prapare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo1_o = []\n",
    "segmentations_o = []\n",
    "\n",
    "for h5_file in tqdm(image_names_train,desc='process dataset: '):\n",
    "\n",
    "    image_file = os.path.join(folder_path, h5_file)\n",
    "    with h5py.File(image_file, \"r\") as f:\n",
    "        echo1_f = f[\"echo1\"][()]  # Shape: (x, y, z)\n",
    "        echo2 = f[\"echo2\"][()]  # Shape: (x, y, z)\n",
    "        segmentations_f = f[\"seg\"][()]  # Shape: (x, y, z, #classes)\n",
    "        \n",
    "    # print(echo1_f.shape[2])\n",
    "    # if echo1_f.shape[2] == 160:\n",
    "    echo1_o.append(echo1_f)\n",
    "    segmentations_o.append(segmentations_f)\n",
    "\n",
    "\n",
    "echo1_o = np.concatenate(echo1_o,axis=2)\n",
    "segmentations_o = np.concatenate(segmentations_o,axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "batch_size = 40\n",
    "sum_epoch = 100\n",
    "pretrained = False\n",
    "lr = 1e-3\n",
    "CHECKPOINT = False\n",
    "dice_loss = False\n",
    "save_modelpath = \"./save_model/\"\n",
    "save_checkPath = './checkPoint_Transnet_bce/'\n",
    "save_path = os.path.join(save_modelpath, 'transnet_model_100_BCE.pth')\n",
    "scalar_name = \"Transnet_aver_Loss_bce\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransNet()\n",
    "\n",
    "if not pretrained:\n",
    "    weights_init(model)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(os.path.join('./save_model/unet32_model_100.pth'),map_location=DEVICE))\n",
    "    print(\"Load model success! \")\n",
    "\n",
    "print(model)\n",
    "print('------------------')\n",
    "# print_model_info(model)\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr, weight_decay=5e-3)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.94)\n",
    "start_epoch = 0\n",
    "\n",
    "if CHECKPOINT:\n",
    "    path_checkpoint = os.path.join(save_checkPath, 'ckpt_best_120.pth')\n",
    "    checkpoint = torch.load(path_checkpoint)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = echo1_o.shape[2]\n",
    "x_chunks = np.array_split(echo1_o, total / batch_size, axis=2)\n",
    "segmentation_chunks = np.array_split(segmentations_o, total / batch_size, axis=2)\n",
    "\n",
    "tensorboardPath = \"./runs\"\n",
    "if not os.path.isdir(tensorboardPath):\n",
    "    os.mkdir(tensorboardPath)\n",
    "writer = SummaryWriter(log_dir=tensorboardPath)\n",
    "\n",
    "for epoch in range(start_epoch, sum_epoch):\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    print('Start Train')\n",
    "\n",
    "    bat = 0\n",
    "    with tqdm(total=len(x_chunks), desc=f'Epoch {epoch+1}/{sum_epoch}', postfix=dict) as pbar:\n",
    "        for chunk in x_chunks:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                chunk =  torch.as_tensor(chunk).float().to(DEVICE)\n",
    "                chunk = (chunk - chunk.mean()) / chunk.std()\n",
    "\n",
    "                chunk = chunk.permute(2, 0, 1)  #(B,H,W) (40,512,512)\n",
    "                chunk = chunk.unsqueeze(1)      # add a channel dimension (B,C,H,W) (40,1,512,512)\n",
    "                \n",
    "                segmentation = segmentation_chunks[bat]                                                            #[512,512,40,6]\n",
    "                segmentation = collect_mask(segmentation, (0, 1, (2, 3), (4, 5)), out_channel_first=False)         #[512,512,40,4]\n",
    "                # segmentation = oF.one_hot_to_categorical(segmentation, channel_dim=-1)   #label  [1 1 1 ... 2 2 2]   [512,512,40]\n",
    "\n",
    "                segmentation =  torch.as_tensor(segmentation).to(DEVICE)\n",
    "                segmentation = segmentation.permute(2, 0, 1, 3)             #[40,512,512,4]\n",
    "\n",
    "            pre_out = model(chunk)                                          #[40,4,512,512]\n",
    "            # pre_out = oF.pred_to_categorical(pre, activation='sigmoid')   # [40,512,512]\n",
    "\n",
    "            if dice_loss:\n",
    "                loss = Dice_Loss(pre_out, segmentation)\n",
    "            else:\n",
    "                loss = BCE_Loss(pre_out, segmentation)\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            bat = bat + 1\n",
    "            pbar.set_postfix(**{\n",
    "            'cur_loss': loss.item(),\n",
    "            'averange_loss': total_loss / bat\n",
    "            })\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    writer.add_scalars(scalar_name, {\"Train\": total_loss / bat }, epoch)\n",
    "\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch':(epoch+1),\n",
    "            'model':model.state_dict(),\n",
    "            'optimizer':optimizer.state_dict(),\n",
    "            'lr_scheduler':lr_scheduler.state_dict()}\n",
    "        if not os.path.isdir(save_checkPath):\n",
    "            os.mkdir(save_checkPath)\n",
    "        torch.save(checkpoint,save_checkPath+'/ckpt_best_%s.pth'%(str(epoch+1)))\n",
    "\n",
    "# Save  \n",
    "# save_path = os.path.join(save_modelpath, 'unet32_model_200_BCE.pth')\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from skimage.measure import find_contours\n",
    "import scipy\n",
    "\n",
    "# Dice Similarity Coefficient (DSC)\n",
    "def dice_similarity_coefficient(pred, targ):\n",
    "    intersection = np.sum(pred * targ)\n",
    "    return (2. * intersection) / (np.sum(pred) + np.sum(targ))\n",
    "\n",
    "# def dice_similarity_coefficient_new(pred, targ, cls):\n",
    "#     intersection = np.sum(pred * targ)\n",
    "#     return (2. * intersection) / (np.sum(pred) + np.sum(targ))\n",
    "\n",
    "# # Average Symmetric Surface Distance (ASSD)\n",
    "# def average_symmetric_surface_distance(pred, targ):\n",
    "#     # ASSD requires surface extraction which is not trivial. This is a placeholder for the correct implementation.\n",
    "#     # The following uses directed Hausdorff as a rough approximation, but this is not accurate for real ASSD calculations.\n",
    "#     u_hausdorff = directed_hausdorff(pred, targ)[0]\n",
    "#     v_hausdorff = directed_hausdorff(targ, pred)[0]\n",
    "#     return (u_hausdorff + v_hausdorff) / 2.0\n",
    "\n",
    "def calculate_surface_distances(mask1, mask2, spacing):\n",
    "    \"\"\"\n",
    "    Calculate the distances from the surface of mask1 to mask2.\n",
    "    \"\"\"\n",
    "    contours_mask1 = find_contours(mask1, level=0.5, fully_connected='low')\n",
    "    contours_mask2 = find_contours(mask2, level=0.5, fully_connected='low')\n",
    "\n",
    "    if len(contours_mask1) == 0 or len(contours_mask2) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    # Convert contours to a more straightforward list of points\n",
    "    surface_pts_mask1 = np.vstack(contours_mask1).astype(np.float32) * spacing\n",
    "    surface_pts_mask2 = np.vstack(contours_mask2).astype(np.float32) * spacing\n",
    "\n",
    "    distances_mask1_to_mask2 = scipy.spatial.distance.cdist(surface_pts_mask1, surface_pts_mask2)\n",
    "    distances_mask2_to_mask1 = scipy.spatial.distance.cdist(surface_pts_mask2, surface_pts_mask1)\n",
    "\n",
    "    return distances_mask1_to_mask2, distances_mask2_to_mask1\n",
    "\n",
    "def average_symmetric_surface_distance(pred, targ, spacing=(1.0, 1.0, 1.0)):\n",
    "    \"\"\"\n",
    "    Calculate the Average Symmetric Surface Distance (ASSD) between the predicted and target masks.\n",
    "    :param pred: Predicted mask\n",
    "    :param targ: Target mask\n",
    "    :param spacing: The physical spacing between data points in the mask (defaults to 1 in all dimensions)\n",
    "    \"\"\"\n",
    "    assert pred.shape == targ.shape\n",
    "    assd_per_slice = []\n",
    "    for i in range(pred.shape[0]):  # Loop over each slice\n",
    "        distances_pred_to_targ, distances_targ_to_pred = calculate_surface_distances(pred[i], targ[i], spacing)\n",
    "        if distances_pred_to_targ.size == 0 or distances_targ_to_pred.size == 0:\n",
    "            continue  # Skip slices with no contours\n",
    "        assd_per_slice.append(np.mean(np.concatenate([distances_pred_to_targ.min(axis=1), distances_targ_to_pred.min(axis=0)])))\n",
    "    return np.mean(assd_per_slice) if assd_per_slice else np.nan\n",
    "\n",
    "# Volumetric Overlap Error (VOE)\n",
    "def volumetric_overlap_error(pred, targ):\n",
    "    intersection = np.sum(pred * targ)\n",
    "    union = np.sum(pred) + np.sum(targ) - intersection\n",
    "    return 1 - (intersection / union)\n",
    "\n",
    "# Coefficient of Variation (CV)\n",
    "def coefficient_of_variation(image):\n",
    "    return np.std(image) / np.mean(image)\n",
    "\n",
    "# Function to compute the average of all metrics across all classes\n",
    "def compute_average_metrics(pred, targ):\n",
    "    num_classes = pred.shape[-1]\n",
    "    dsc_scores = [dice_similarity_coefficient(pred[..., i], targ[..., i]) for i in range(num_classes)]\n",
    "    assd_scores = [average_symmetric_surface_distance(pred[..., i], targ[..., i]) for i in range(num_classes)]\n",
    "    voe_scores = [volumetric_overlap_error(pred[..., i], targ[..., i]) for i in range(num_classes)]\n",
    "    cv_scores = [coefficient_of_variation(pred[..., i]) for i in range(num_classes)]\n",
    "\n",
    "    avg_dsc = np.mean(dsc_scores)\n",
    "    avg_assd = np.mean(assd_scores)\n",
    "    avg_voe = np.mean(voe_scores)\n",
    "    avg_cv = np.mean(cv_scores)\n",
    "\n",
    "    return avg_dsc, avg_assd, avg_voe, avg_cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Eval U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = True\n",
    "CHECKPOINT = False\n",
    "save_modelpath = \"./save_model/\"\n",
    "save_checkPath = './checkPoint_model_bce/'\n",
    "save_path = os.path.join(save_modelpath, 'unet32_model_100_BCE.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet_32()\n",
    "\n",
    "if not pretrained:\n",
    "    weights_init(model)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(os.path.join(save_path),map_location=DEVICE))\n",
    "    print(\"Load model success! \")\n",
    "\n",
    "print(model)\n",
    "print('------------------')\n",
    "model = model.cuda()\n",
    "\n",
    "if CHECKPOINT:\n",
    "        path_checkpoint = os.path.join(save_checkPath, 'ckpt_best_200.pth')\n",
    "        checkpoint = torch.load(path_checkpoint)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentations_o = []\n",
    "out = []\n",
    "\n",
    "model.eval()\n",
    "# for h5_file in tqdm(image_names_test[20:32],desc='process Test: '):\n",
    "for h5_file in tqdm(image_names_test,desc='process Test: '):\n",
    "    one_pre = []\n",
    "    image_file = os.path.join(folder_path, h5_file)\n",
    "    with h5py.File(image_file, \"r\") as f:\n",
    "        echo1_f = f[\"echo1\"][()]  # Shape: (x, y, z)\n",
    "        echo2 = f[\"echo2\"][()]  # Shape: (x, y, z)\n",
    "        segmentations_f = f[\"seg\"][()]  # Shape: (x, y, z, #classes)\n",
    "    \n",
    "    if echo1_f.shape[2] != 160:\n",
    "        continue\n",
    "    \n",
    "    echo1_f = (echo1_f - echo1_f.mean()) / echo1_f.std()\n",
    "\n",
    "    segmentations_o.append(segmentations_f)    \n",
    "\n",
    "    for i in range(echo1_f.shape[2]):\n",
    "        pic = echo1_f[:,:,i]\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            pic =  torch.as_tensor(pic).float().to('cuda')\n",
    "            pic = pic.unsqueeze(0).unsqueeze(0)       # add a channel dimension (B,C,H,W) (1,1,512,512)\n",
    "\n",
    "            pre_out = model(pic)                      \n",
    "\n",
    "            # print(pre_out[0][0])\n",
    "            pre_out = oF.pred_to_categorical(pre_out, activation='sigmoid')  # (1, 512, 512)\n",
    "            pre_out = oF.categorical_to_one_hot(pre_out,num_categories=4)    # (1, 4, 512, 512)\n",
    "            one_pre.append(pre_out)\n",
    "    \n",
    "    \n",
    "    one_pre = torch.concat(one_pre, dim=0)    # (160, 4, 512, 512)\n",
    "    # one_pre = one_pre.permute(0,3,4,1,2)\n",
    "    one_pre = one_pre.permute(2,3,0,1)\n",
    "    out.append(one_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_o = []\n",
    "for outt in tqdm(out, desc='process Out: '):\n",
    "    outt = outt.cpu().numpy()\n",
    "    result_o.append(outt)\n",
    "\n",
    "\n",
    "print('result_o stack')\n",
    "result_o = np.stack(result_o)\n",
    "\n",
    "segs = []\n",
    "for seg in tqdm(segmentations_o,desc='process Seg: '):\n",
    "    seg = collect_mask(seg, (0, 1, (2, 3), (4, 5)), out_channel_first=False) \n",
    "    segs.append(seg)\n",
    "\n",
    "print('segs stack')\n",
    "result_true_o = np.stack(segs)\n",
    "\n",
    "\n",
    "print(result_true_o.shape)\n",
    "print(result_o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = result_o.shape[0]\n",
    "num_classes = result_true_o.shape[-1]\n",
    "# Initialize lists to store metric scores for each sample\n",
    "dsc_list, assd_list, voe_list, cv_list = [], [], [], []\n",
    "\n",
    "# Compute metrics for each sample\n",
    "for n in tqdm([ i for i in range(num_samples)], desc='process eval: '):\n",
    "    dsc_scores = [dice_similarity_coefficient(result_o[n, ..., i], result_true_o[n, ..., i]) for i in range(num_classes)]\n",
    "    # assd_scores = [average_symmetric_surface_distance(pred[n, ..., i], targ[n, ..., i]) for i in range(num_classes)]\n",
    "    voe_scores = [volumetric_overlap_error(result_o[n, ..., i], result_true_o[n, ..., i]) for i in range(num_classes)]\n",
    "    cv_scores = [coefficient_of_variation(result_o[n, ..., i]) for i in range(num_classes)]\n",
    "    \n",
    "    dsc_list.append(np.mean(dsc_scores))\n",
    "    # assd_list.append(np.mean(assd_scores))\n",
    "    voe_list.append(np.mean(voe_scores))\n",
    "    cv_list.append(np.mean(cv_scores))\n",
    "\n",
    "# Calculate average across all samples\n",
    "avg_dsc = np.mean(dsc_list)\n",
    "# avg_assd = np.mean(assd_list)\n",
    "avg_voe = np.mean(voe_list)\n",
    "avg_cv = np.mean(cv_list)\n",
    "\n",
    "print(\"Eval U-Net:\")\n",
    "print(avg_dsc,avg_voe,avg_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Eval V-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = True\n",
    "CHECKPOINT = False\n",
    "save_modelpath = \"./save_model/\"\n",
    "save_checkPath = './checkPoint_Vmodel_bce/'\n",
    "save_path = os.path.join(save_modelpath, 'Vnet80_model_100_BCE.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VNet_80()\n",
    "\n",
    "if not pretrained:\n",
    "    weights_init(model)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(os.path.join(save_path),map_location=DEVICE))\n",
    "    print(\"Load model success! \")\n",
    "\n",
    "print(model)\n",
    "print('------------------')\n",
    "model = model.cuda()\n",
    "\n",
    "if CHECKPOINT:\n",
    "        path_checkpoint = os.path.join(save_checkPath, 'ckpt_best_200.pth')\n",
    "        checkpoint = torch.load(path_checkpoint)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_o = []\n",
    "result_true_o = []\n",
    "# for h5_file in tqdm(image_names_test,desc='process dataset: '):\n",
    "for h5_file in tqdm(image_names_test,desc='process Test: '):\n",
    "\n",
    "        image_file = os.path.join(folder_path, h5_file)\n",
    "        with h5py.File(image_file, \"r\") as f:\n",
    "            echo1_f = f[\"echo1\"][()]  # Shape: (x, y, z)\n",
    "            echo2 = f[\"echo2\"][()]  # Shape: (x, y, z)\n",
    "            segmentations_f = f[\"seg\"][()]  # Shape: (x, y, z, #classes)\n",
    "            \n",
    "        # print(echo1_f.shape[2])\n",
    "        if echo1_f.shape[2] == 160:\n",
    "            echo1_o.append(echo1_f)\n",
    "            segmentations_o.append(segmentations_f)\n",
    "        else:\n",
    "             continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            x_chunks = np.array_split(echo1_f, 4, axis=2)\n",
    "            # segmentation_chunks = np.array_split(segmentations_o[bat], 4, axis=2)\n",
    "\n",
    "        sl = 0\n",
    "        result = []\n",
    "        # result_true = []\n",
    "        for x_chunk in x_chunks:\n",
    "            with torch.no_grad():\n",
    "                chunk =  torch.as_tensor(x_chunk).float().to('cuda')\n",
    "                chunk = (chunk - chunk.mean()) / chunk.std()        # (512,512,40)\n",
    "\n",
    "\n",
    "                chunk = chunk.permute(2, 0, 1)                      # (B,H,W) (160,512,512)\n",
    "                chunk = chunk.unsqueeze(0).unsqueeze(0)             # add a channel dimension (B,C,H,W) (1,1,40,512,512)\n",
    "\n",
    "                pre_out = model(chunk)                              # (1,4,40,512,512)\n",
    "                result.append(pre_out)  \n",
    "                # result_true.append(segmentation)\n",
    "                sl += sl\n",
    "\n",
    "        result = torch.concat(result, dim=2)    #(1,4,160,512,512)\n",
    "        # result_true = torch.concat(result_true, dim=3)\n",
    "        result_o.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('precess Out')\n",
    "result_o = torch.concat(result_o, dim=0)    # (5,4,160,512,512)\n",
    "result_o =result_o.permute(0,3,4,2,1)       # (5,512,512,160,4)\n",
    "result_o = result_o.cpu()\n",
    "result_o = result_o.numpy()\n",
    "\n",
    "segs = []\n",
    "for seg in tqdm(segmentations_o, desc='process Seg: '):\n",
    "    seg = collect_mask(seg, (0, 1, (2, 3), (4, 5)), out_channel_first=False)  # (512,512,160,6) to (512,512,160,4)\n",
    "    segs.append(seg)\n",
    "\n",
    "result_true_o = np.stack(segs)             # (5,512,512,160,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = result_o.shape[0]\n",
    "num_classes = result_true_o.shape[-1]\n",
    "# Initialize lists to store metric scores for each sample\n",
    "dsc_list, assd_list, voe_list, cv_list = [], [], [], []\n",
    "\n",
    "# Compute metrics for each sample\n",
    "for n in tqdm([ i for i in range(num_samples)], desc='process eval: '):\n",
    "    dsc_scores = [dice_similarity_coefficient(result_o[n, ..., i], result_true_o[n, ..., i]) for i in range(num_classes)]\n",
    "    # assd_scores = [average_symmetric_surface_distance(pred[n, ..., i], targ[n, ..., i]) for i in range(num_classes)]\n",
    "    voe_scores = [volumetric_overlap_error(result_o[n, ..., i], result_true_o[n, ..., i]) for i in range(num_classes)]\n",
    "    cv_scores = [coefficient_of_variation(result_o[n, ..., i]) for i in range(num_classes)]\n",
    "    \n",
    "    dsc_list.append(np.mean(dsc_scores))\n",
    "    # assd_list.append(np.mean(assd_scores))\n",
    "    voe_list.append(np.mean(voe_scores))\n",
    "    cv_list.append(np.mean(cv_scores))\n",
    "\n",
    "# Calculate average across all samples\n",
    "avg_dsc = np.mean(dsc_list)\n",
    "# avg_assd = np.mean(assd_list)\n",
    "avg_voe = np.mean(voe_list)\n",
    "avg_cv = np.mean(cv_list)\n",
    "\n",
    "print(\"Eval V-Net:\")\n",
    "print(avg_dsc,avg_voe,avg_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Eval TransNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = True\n",
    "CHECKPOINT = False\n",
    "save_modelpath = \"./save_model/\"\n",
    "save_checkPath = './checkPoint_Transnet_bce/'\n",
    "save_path = os.path.join(save_modelpath, 'transnet_model_100_BCE.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransNet()\n",
    "\n",
    "if not pretrained:\n",
    "    weights_init(model)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(os.path.join(save_path),map_location=DEVICE))\n",
    "    print(\"Load model success! \")\n",
    "\n",
    "print(model)\n",
    "print('------------------')\n",
    "model = model.cuda()\n",
    "\n",
    "if CHECKPOINT:\n",
    "        path_checkpoint = os.path.join(save_checkPath, 'ckpt_best_200.pth')\n",
    "        checkpoint = torch.load(path_checkpoint)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentations_o = []\n",
    "out = []\n",
    "\n",
    "model.eval()\n",
    "# for h5_file in tqdm(image_names_test[20:32],desc='process Test: '):\n",
    "for h5_file in tqdm(image_names_test,desc='process Test: '):\n",
    "    one_pre = []\n",
    "    image_file = os.path.join(folder_path, h5_file)\n",
    "    with h5py.File(image_file, \"r\") as f:\n",
    "        echo1_f = f[\"echo1\"][()]  # Shape: (x, y, z)\n",
    "        echo2 = f[\"echo2\"][()]  # Shape: (x, y, z)\n",
    "        segmentations_f = f[\"seg\"][()]  # Shape: (x, y, z, #classes)\n",
    "    \n",
    "    if echo1_f.shape[2] != 160:\n",
    "        continue\n",
    "    \n",
    "    echo1_f = (echo1_f - echo1_f.mean()) / echo1_f.std()\n",
    "\n",
    "    segmentations_o.append(segmentations_f)    \n",
    "\n",
    "    for i in range(echo1_f.shape[2]):\n",
    "        pic = echo1_f[:,:,i]\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            pic =  torch.as_tensor(pic).float().to('cuda')\n",
    "            pic = pic.unsqueeze(0).unsqueeze(0)       # add a channel dimension (B,C,H,W) (1,1,512,512)\n",
    "\n",
    "            pre_out = model(pic)                      # (1, 4, 512, 512)\n",
    "            \n",
    "\n",
    "            pre_out = oF.pred_to_categorical(pre_out, activation='sigmoid')  # (1, 512, 512)\n",
    "\n",
    "            pre_out = oF.categorical_to_one_hot(pre_out,num_categories=4)    # (1, 4, 512, 512)\n",
    "\n",
    "            one_pre.append(pre_out)\n",
    "    \n",
    "    \n",
    "    one_pre = torch.concat(one_pre, dim=0)    # (160, 4, 512, 512)\n",
    "    one_pre = one_pre.permute(2,3,0,1)\n",
    "    out.append(one_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_o = []\n",
    "for outt in tqdm(out, desc='process Out: '):\n",
    "    outt = outt.cpu().numpy()\n",
    "    result_o.append(outt)\n",
    "\n",
    "\n",
    "print('result_o stack')\n",
    "result_o = np.stack(result_o)\n",
    "\n",
    "segs = []\n",
    "for seg in tqdm(segmentations_o,desc='process Seg: '):\n",
    "    seg = collect_mask(seg, (0, 1, (2, 3), (4, 5)), out_channel_first=False) \n",
    "    segs.append(seg)\n",
    "\n",
    "print('segs stack')\n",
    "result_true_o = np.stack(segs)\n",
    "\n",
    "\n",
    "print(result_true_o.shape)\n",
    "print(result_o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = result_o.shape[0]\n",
    "num_classes = result_true_o.shape[-1]\n",
    "# Initialize lists to store metric scores for each sample\n",
    "dsc_list, assd_list, voe_list, cv_list = [], [], [], []\n",
    "\n",
    "# Compute metrics for each sample\n",
    "for n in tqdm([ i for i in range(num_samples)], desc='process eval: '):\n",
    "    dsc_scores = [dice_similarity_coefficient(result_o[n, ..., i], result_true_o[n, ..., i]) for i in range(num_classes)]\n",
    "    # assd_scores = [average_symmetric_surface_distance(pred[n, ..., i], targ[n, ..., i]) for i in range(num_classes)]\n",
    "    voe_scores = [volumetric_overlap_error(result_o[n, ..., i], result_true_o[n, ..., i]) for i in range(num_classes)]\n",
    "    cv_scores = [coefficient_of_variation(result_o[n, ..., i]) for i in range(num_classes)]\n",
    "    \n",
    "    dsc_list.append(np.mean(dsc_scores))\n",
    "    # assd_list.append(np.mean(assd_scores))\n",
    "    voe_list.append(np.mean(voe_scores))\n",
    "    cv_list.append(np.mean(cv_scores))\n",
    "\n",
    "# Calculate average across all samples\n",
    "avg_dsc = np.mean(dsc_list)\n",
    "# avg_assd = np.mean(assd_list)\n",
    "avg_voe = np.mean(voe_list)\n",
    "avg_cv = np.mean(cv_list)\n",
    "\n",
    "print(\"Eval TransNet:\")\n",
    "print(avg_dsc,avg_voe,avg_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "image_file = os.path.join(folder_path, image_names_test[0])\n",
    "with h5py.File(image_file, \"r\") as f:\n",
    "    echo1_f = f[\"echo1\"][()]  # Shape: (x, y, z)\n",
    "    # echo2 = f[\"echo2\"][()]  # Shape: (x, y, z)\n",
    "    segmentations_f = f[\"seg\"][()]  # Shape: (x, y, z, #classes)\n",
    "\n",
    "echo1_f = (echo1_f - echo1_f.mean()) / echo1_f.std()\n",
    "\n",
    "test_data.append(echo1_f)\n",
    "\n",
    "echo1_f = []\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "print(test_data.shape)\n",
    "print(segmentations_f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = TransNet()\n",
    "model_0.load_state_dict(torch.load(os.path.join('./save_model/TransNet_model_100.pth'),map_location=DEVICE))\n",
    "print(\"Load model0 success! \")\n",
    "\n",
    "model_1 = VNet_80()\n",
    "model_1.load_state_dict(torch.load(os.path.join('./save_model/VNet_80_model_100.pth'),map_location=DEVICE))\n",
    "print(\"Load model1 success! \")\n",
    "\n",
    "model_2 = UNet_32()\n",
    "model_2.load_state_dict(torch.load(os.path.join('./save_model/unet32_model_100.pth'),map_location=DEVICE))\n",
    "print(\"Load model2 success! \")\n",
    "\n",
    "\n",
    "model_0 = model_0.to(DEVICE)\n",
    "model_1 = model_1.to(DEVICE)\n",
    "model_2 = model_2.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred = np.zeros((3, 512, 512, 160))\n",
    "for i in range(1):\n",
    "    for j in range(160):\n",
    "        echo1_sl = test_data[i,:,:,j]\n",
    "        echo1_sl = torch.as_tensor(echo1_sl).unsqueeze(0).unsqueeze(0).float().to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model_0({\"image\": echo1_sl})[\"sem_seg_logits\"]\n",
    "            logits = oF.pred_to_categorical(logits, activation='sigmoid')\n",
    "            # logits = oF.categorical_to_one_hot(logits,3,num_categories=4)\n",
    "\n",
    "            # logits = prediction_2\n",
    "            # logits = logits1.permute(0,2,3,1)\n",
    "            # prediction = oF.pred_to_categorical(logits, activation='sigmoid').squeeze(0)\n",
    "\n",
    "            logits = logits.cpu()\n",
    "            pred[0,:,:,j] = logits[0]\n",
    "\n",
    "            logits = model_1({\"image\": echo1_sl})[\"sem_seg_logits\"]\n",
    "            logits = oF.pred_to_categorical(logits, activation='sigmoid')\n",
    "            logits = logits.cpu()\n",
    "            pred[1,:,:,j] = logits[0]\n",
    "\n",
    "            logits = model_2({\"image\": echo1_sl})[\"sem_seg_logits\"]\n",
    "            logits = oF.pred_to_categorical(logits, activation='sigmoid')\n",
    "            logits = logits.cpu()\n",
    "            pred[2,:,:,j] = logits[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = 32\n",
    "E1 = pred[0]  #[512, 512, 160]\n",
    "prediction_0 = E1[:, :, sl]\n",
    "E2 = pred[1]  #[512, 512, 160]\n",
    "prediction_1 = E2[:, :, sl]\n",
    "RSS = pred[2]  #[512, 512, 160]\n",
    "prediction_2 = RSS[:, :, sl]\n",
    "gt_seg_sl = segmentations_f[:, :, sl]                            #[512, 512, 160]\n",
    "input = test_data[0,..., sl]\n",
    "\n",
    "sl = 150\n",
    "E1 = pred[0]  #[512, 512, 160]\n",
    "prediction_01 = E1[:, sl, :]\n",
    "E2 = pred[1]  #[512, 512, 160]\n",
    "prediction_11 = E2[:, sl, :]\n",
    "RSS = pred[2]  #[512, 512, 160]\n",
    "prediction_21 = RSS[:, sl, :]\n",
    "gt_seg_sl_1 = segmentations_f[:, sl, :]                    \n",
    "input_1 = test_data[0, :, sl, :]\n",
    "\n",
    "\n",
    "sl = 190\n",
    "E1 = pred[0]  #[512, 512, 160]\n",
    "prediction_02 = E1[sl, :, :]\n",
    "E2 = pred[1]  #[512, 512, 160]\n",
    "prediction_12 = E2[sl, :, :]\n",
    "RSS = pred[2]  #[512, 512, 160]\n",
    "prediction_22 = RSS[sl, :, :]\n",
    "gt_seg_sl_2 = segmentations_f[sl, :, :]         \n",
    "input_2 = test_data[0, sl, :, :]\n",
    "\n",
    "\n",
    "_, axs = plt.subplots(3, 5, figsize=(15,12))\n",
    "for idx, (data, title) in enumerate([\n",
    "  (input.squeeze(), \"Input\"), (prediction_0, \"TransUNet\"),(prediction_1, \"Vnet\"), (prediction_2, \"Unet \"),  (gt_seg_sl, \"Ground truth\")\n",
    "]):\n",
    "    ax = axs[0, idx]\n",
    "    ax.imshow(data.squeeze(), cmap=\"gray\" if idx == 0 else None)\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "for idx, (data, title) in enumerate([\n",
    "  (input_1.squeeze(), \"Input\"), (prediction_01, \"TransUNet\"),(prediction_11, \"Vnet\"), (prediction_21, \"Unet \"),  (gt_seg_sl_1, \"Ground truth\")\n",
    "]):\n",
    "    ax = axs[1, idx]\n",
    "    ax.imshow(data.squeeze(), cmap=\"gray\" if idx == 0 else None)\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "for idx, (data, title) in enumerate([\n",
    "  (input_2.squeeze(), \"Input\"), (prediction_02, \"TransUNet\"),(prediction_12, \"Vnet\"), (prediction_22, \"Unet \"),  (gt_seg_sl_2, \"Ground truth\")\n",
    "]):\n",
    "    ax = axs[2, idx]\n",
    "    ax.imshow(data.squeeze(), cmap=\"gray\" if idx == 0 else None)\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
